{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# import libs\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peyGYX-YDaRK",
        "outputId": "11983b34-c590-4ddf-cfb1-fb56d9806b13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drZqTmlJCXKd"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"Beans. I was trying to explain to somebody as we were flying in, that’s corn.\n",
        "That’s beans. And they were very impressed at my agricultural knowledge.\n",
        "Please give it up for Amaury once again for that outstanding introduction. I\n",
        "have a bunch of good friends here today, including somebody who I served\n",
        "with, who is one of the finest senators in the country, and we’re lucky to have\n",
        "him, your Senator, Dick Durbin is here. I also noticed, by the way, former\n",
        "Governor Edgar here, who I haven’t seen in a long time, and somehow he\n",
        "has not aged and I have. And it’s great to see you, Governor. I want to thank\n",
        "President Killeen and everybody at the U of I System for making it possible\n",
        "for me to be here today. And I am deeply honored at the Paul Douglas Award\n",
        "that is being given to me. He is somebody who set the path for so much\n",
        "outstanding public service here in Illinois. Now, I want to start by addressing\n",
        "the elephant in the room. I know people are still wondering why I didn’t speak\n",
        "at the commencement.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pre process\n",
        "\n",
        "# lower all text char\n",
        "t_lower=text.lower()\n",
        "\n",
        "# tokenize to sent\n",
        "sentences =sent_tokenize(t_lower)\n",
        "\n",
        "list_sen = []\n",
        "sen_keep = \"\"\n",
        "\n",
        "for sen in sentences:\n",
        "  # tokenize to words\n",
        "  tokens=word_tokenize(sen)\n",
        "\n",
        "  # delete punct\n",
        "  no_punct=[word for word in tokens if word not in string.punctuation]\n",
        "\n",
        "  # lemmatize\n",
        "  lemma=WordNetLemmatizer()\n",
        "  token_lemma=[lemma.lemmatize(word) for word in no_punct]\n",
        "\n",
        "  # delete stopwords\n",
        "  stop_words=set(stopwords.words(\"english\"))\n",
        "  no_stopword=[word for word in token_lemma if word not in stop_words]\n",
        "\n",
        "  # delete ’\n",
        "  final_text=[word for word in no_stopword if word != '’']\n",
        "\n",
        "  sen_keep = \" \".join(final_text)\n",
        "  list_sen.append(sen_keep)\n",
        "  sen_keep = ''\n"
      ],
      "metadata": {
        "id": "AT8QEGPFDWPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(list_sen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCelAGjXFxoR",
        "outputId": "c43d8ed9-3f6c-49bb-c9e3-6a2624d153e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['bean', 'wa trying explain somebody flying corn', 'bean', 'impressed agricultural knowledge', 'please give amaury outstanding introduction', 'bunch good friend today including somebody served one finest senator country lucky senator dick durbin', 'also noticed way former governor edgar seen long time somehow ha aged', 'great see governor', 'want thank president killeen everybody u system making possible today', 'deeply honored paul douglas award given', 'somebody set path much outstanding public service illinois', 'want start addressing elephant room', 'know people still wondering speak commencement']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# apply countvectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vector=CountVectorizer(binary=True)\n",
        "x = vector.fit_transform(list_sen)"
      ],
      "metadata": {
        "id": "UmUw7bINHIeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bm_XufhxHuT3",
        "outputId": "4c132ac6-2a9f-42a7-cf47-154c190e5484"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(13, 72)\n"
          ]
        }
      ]
    }
  ]
}